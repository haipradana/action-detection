{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "ename": "SyntaxError",
          "evalue": "invalid syntax (2504023825.py, line 5)",
          "output_type": "error",
          "traceback": [
            "\u001b[1;36m  Cell \u001b[1;32mIn[1], line 5\u001b[1;36m\u001b[0m\n\u001b[1;33m    - **Dataset**: 5 videos, 295 clips, 5 action classes\u001b[0m\n\u001b[1;37m      ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "# üéØ MERL Shopping Action Recognition Training\n",
        "## Simple but Effective Approach for 5 Videos Dataset\n",
        "\n",
        "### üìã Overview:\n",
        "- **Dataset**: 5 videos, 295 clips, 5 action classes\n",
        "- **Approach**: TimeDistributed CNN + LSTM for temporal modeling\n",
        "- **Strategy**: No validation split (maximize training data)\n",
        "- **Goal**: Proof of concept with good performance\n",
        "\n",
        "### üé¨ Action Classes:\n",
        "1. **Reach To Shelf** - Reaching towards shelf\n",
        "2. **Retract From Shelf** - Moving hand back from shelf  \n",
        "3. **Hand In Shelf** - Hand inside shelf area\n",
        "4. **Inspect Product** - Looking at/examining product\n",
        "5. **Inspect Shelf** - Looking at shelf contents\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## üì¶ 1. Setup and Dependencies\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "%pip install tensorflow opencv-python scipy pandas numpy matplotlib scikit-learn\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import (\n",
        "    TimeDistributed, Conv2D, MaxPooling2D, Flatten, Dense, Dropout, \n",
        "    LSTM, BatchNormalization, GlobalAveragePooling2D\n",
        ")\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.utils import Sequence\n",
        "import gc\n",
        "\n",
        "print(f\"TensorFlow version: {tf.__version__}\")\n",
        "print(f\"Keras version: {tf.keras.__version__}\")\n",
        "\n",
        "# GPU setup\n",
        "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
        "print(f\"üî• Found {len(physical_devices)} GPU(s)\")\n",
        "for gpu in physical_devices:\n",
        "    tf.config.experimental.set_memory_growth(gpu, True)\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## üìä 2. Dataset Verification\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verify dataset structure\n",
        "print(\"üìÅ Dataset Structure Check:\")\n",
        "print(f\"Working directory: {os.getcwd()}\")\n",
        "print(\"\\nFolders:\")\n",
        "for folder in ['clips', 'dataframes', 'flow_clips']:\n",
        "    if os.path.exists(folder):\n",
        "        print(f\"‚úÖ {folder}/ exists\")\n",
        "    else:\n",
        "        print(f\"‚ùå {folder}/ missing\")\n",
        "\n",
        "# Check dataframes\n",
        "dataframe_files = [f for f in os.listdir('dataframes') if f.endswith('.csv')]\n",
        "print(f\"\\nüìÑ Found {len(dataframe_files)} dataframe files:\")\n",
        "for df_file in sorted(dataframe_files):\n",
        "    print(f\"   - {df_file}\")\n",
        "\n",
        "# Check clips\n",
        "clips_folders = [f for f in os.listdir('clips') if os.path.isdir(f'clips/{f}')]\n",
        "print(f\"\\nüé¨ Found {len(clips_folders)} video folders:\")\n",
        "for folder in sorted(clips_folders):\n",
        "    clip_files = len([f for f in os.listdir(f'clips/{folder}') if f.endswith('.npy')])\n",
        "    print(f\"   - {folder}: {clip_files} clips\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze class distribution\n",
        "class_distribution = [0] * 5\n",
        "total_clips = 0\n",
        "all_clip_data = []\n",
        "\n",
        "print(\"üìä Analyzing class distribution...\")\n",
        "for df_file in sorted(dataframe_files):\n",
        "    df = pd.read_csv(f'dataframes/{df_file}')\n",
        "    total_clips += len(df)\n",
        "    \n",
        "    for _, row in df.iterrows():\n",
        "        video_num = df_file.split('_')[1].split('.')[0]\n",
        "        clip_path = f\"clips/video_{video_num}/{row['name']}.npy\"\n",
        "        class_label = int(row['class']) - 1  # Convert to 0-indexed\n",
        "        \n",
        "        all_clip_data.append((clip_path, class_label))\n",
        "        class_distribution[class_label] += 1\n",
        "    \n",
        "    print(f\"   üìÑ {df_file}: {len(df)} clips\")\n",
        "\n",
        "# Action class names\n",
        "action_classes = [\n",
        "    'Reach To Shelf',\n",
        "    'Retract From Shelf', \n",
        "    'Hand In Shelf',\n",
        "    'Inspect Product',\n",
        "    'Inspect Shelf'\n",
        "]\n",
        "\n",
        "print(f\"\\nüìà Class Distribution:\")\n",
        "for i, (class_name, count) in enumerate(zip(action_classes, class_distribution)):\n",
        "    percentage = (count / total_clips * 100)\n",
        "    print(f\"   {i}: {class_name:<20} = {count:3d} clips ({percentage:5.1f}%)\")\n",
        "\n",
        "print(f\"\\nüìä Total clips: {total_clips}\")\n",
        "print(f\"üìä Balance std: {np.std([c/total_clips*100 for c in class_distribution]):.2f}%\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## üîÑ 3. Data Generator\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MerlActionDataGenerator(Sequence):\n",
        "    \"\"\"\n",
        "    Simple and efficient data generator for MERL Shopping dataset\n",
        "    \"\"\"\n",
        "    def __init__(self, clip_data, sequence_length=10, batch_size=4, \n",
        "                 target_size=(224, 224), shuffle=True, augment=False):\n",
        "        self.clip_data = clip_data\n",
        "        self.sequence_length = sequence_length\n",
        "        self.batch_size = batch_size\n",
        "        self.target_size = target_size\n",
        "        self.shuffle = shuffle\n",
        "        self.augment = augment\n",
        "        self.indices = np.arange(len(self.clip_data))\n",
        "        \n",
        "        if self.shuffle:\n",
        "            np.random.shuffle(self.indices)\n",
        "            \n",
        "        print(f\"üìä Data Generator initialized:\")\n",
        "        print(f\"   - Total clips: {len(self.clip_data)}\")\n",
        "        print(f\"   - Sequence length: {self.sequence_length}\")\n",
        "        print(f\"   - Batch size: {self.batch_size}\")\n",
        "        print(f\"   - Batches per epoch: {len(self)}\")\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.clip_data) // self.batch_size\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        batch_indices = self.indices[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
        "        \n",
        "        batch_clips = []\n",
        "        batch_labels = []\n",
        "        \n",
        "        for i in batch_indices:\n",
        "            clip_path, label = self.clip_data[i]\n",
        "            \n",
        "            try:\n",
        "                # Load clip\n",
        "                clip = np.load(clip_path)\n",
        "                \n",
        "                # Adjust sequence length\n",
        "                if len(clip) >= self.sequence_length:\n",
        "                    # Take middle portion if clip is longer\n",
        "                    start_idx = (len(clip) - self.sequence_length) // 2\n",
        "                    clip = clip[start_idx:start_idx + self.sequence_length]\n",
        "                else:\n",
        "                    # Repeat frames if clip is shorter\n",
        "                    repeat_factor = self.sequence_length // len(clip) + 1\n",
        "                    clip = np.tile(clip, (repeat_factor, 1, 1, 1))[:self.sequence_length]\n",
        "                \n",
        "                # Simple augmentation\n",
        "                if self.augment and np.random.random() > 0.5:\n",
        "                    clip = np.flip(clip, axis=2)  # Horizontal flip\n",
        "                \n",
        "                batch_clips.append(clip)\n",
        "                batch_labels.append(label)\n",
        "                \n",
        "            except Exception as e:\n",
        "                print(f\"‚ö†Ô∏è Error loading {clip_path}: {e}\")\n",
        "                # Use dummy data if loading fails\n",
        "                dummy_clip = np.zeros((self.sequence_length, 224, 224, 3))\n",
        "                batch_clips.append(dummy_clip)\n",
        "                batch_labels.append(0)\n",
        "        \n",
        "        return np.array(batch_clips), np.array(batch_labels)\n",
        "    \n",
        "    def on_epoch_end(self):\n",
        "        if self.shuffle:\n",
        "            np.random.shuffle(self.indices)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create data generator\n",
        "print(\"üîÑ Creating data generator...\")\n",
        "train_generator = MerlActionDataGenerator(\n",
        "    clip_data=all_clip_data,\n",
        "    sequence_length=10,  # 10 frames per sequence\n",
        "    batch_size=4,        # Small batch size for memory efficiency\n",
        "    target_size=(224, 224),\n",
        "    shuffle=True,\n",
        "    augment=True\n",
        ")\n",
        "\n",
        "# Test the generator\n",
        "print(\"\\nüß™ Testing data generator...\")\n",
        "try:\n",
        "    X_test, y_test = train_generator[0]\n",
        "    print(f\"‚úÖ Batch shape: X={X_test.shape}, y={y_test.shape}\")\n",
        "    print(f\"   X range: [{X_test.min():.3f}, {X_test.max():.3f}]\")\n",
        "    print(f\"   Labels: {y_test}\")\n",
        "    print(f\"   Unique labels: {np.unique(y_test)}\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Generator test failed: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## üèóÔ∏è 4. Model Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_merl_action_model(sequence_length=10, img_height=224, img_width=224, num_classes=5):\n",
        "    \"\"\"\n",
        "    Create a simple but effective model for action recognition\n",
        "    Architecture: TimeDistributed CNN + LSTM + Dense\n",
        "    \"\"\"\n",
        "    model = Sequential([\n",
        "        # CNN feature extractor for each frame\n",
        "        TimeDistributed(Conv2D(32, (3, 3), activation='relu', padding='same'), \n",
        "                       input_shape=(sequence_length, img_height, img_width, 3)),\n",
        "        TimeDistributed(MaxPooling2D((2, 2))),\n",
        "        \n",
        "        TimeDistributed(Conv2D(64, (3, 3), activation='relu', padding='same')),\n",
        "        TimeDistributed(MaxPooling2D((2, 2))),\n",
        "        \n",
        "        TimeDistributed(Conv2D(128, (3, 3), activation='relu', padding='same')),\n",
        "        TimeDistributed(MaxPooling2D((2, 2))),\n",
        "        \n",
        "        TimeDistributed(Conv2D(256, (3, 3), activation='relu', padding='same')),\n",
        "        TimeDistributed(GlobalAveragePooling2D()),\n",
        "        TimeDistributed(BatchNormalization()),\n",
        "        TimeDistributed(Dropout(0.3)),\n",
        "        \n",
        "        # LSTM for temporal modeling\n",
        "        LSTM(128, return_sequences=False, dropout=0.3, recurrent_dropout=0.3),\n",
        "        \n",
        "        # Classification head\n",
        "        Dense(64, activation='relu'),\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.5),\n",
        "        Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "    \n",
        "    return model\n",
        "\n",
        "# Create model\n",
        "print(\"üèóÔ∏è Creating model...\")\n",
        "model = create_merl_action_model(\n",
        "    sequence_length=10,\n",
        "    img_height=224,\n",
        "    img_width=224,\n",
        "    num_classes=5\n",
        ")\n",
        "\n",
        "# Compile model\n",
        "model.compile(\n",
        "    optimizer=Adam(learning_rate=0.0001),\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "print(\"\\nüìã Model Summary:\")\n",
        "model.summary()\n",
        "\n",
        "# Count parameters\n",
        "trainable_params = model.count_params()\n",
        "print(f\"\\nüìä Total trainable parameters: {trainable_params:,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## üéØ 5. Training Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate class weights for balanced training\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "# Get all labels\n",
        "all_labels = [label for _, label in all_clip_data]\n",
        "unique_classes = np.unique(all_labels)\n",
        "\n",
        "# Compute class weights\n",
        "class_weights = compute_class_weight(\n",
        "    'balanced',\n",
        "    classes=unique_classes,\n",
        "    y=all_labels\n",
        ")\n",
        "\n",
        "class_weight_dict = dict(zip(unique_classes, class_weights))\n",
        "\n",
        "print(\"‚öñÔ∏è Class weights for balanced training:\")\n",
        "for i, weight in class_weight_dict.items():\n",
        "    print(f\"   Class {i} ({action_classes[i]}): {weight:.3f}\")\n",
        "\n",
        "# Training callbacks\n",
        "callbacks = [\n",
        "    EarlyStopping(\n",
        "        monitor='loss',\n",
        "        patience=10,\n",
        "        restore_best_weights=True,\n",
        "        verbose=1\n",
        "    ),\n",
        "    ReduceLROnPlateau(\n",
        "        monitor='loss',\n",
        "        factor=0.5,\n",
        "        patience=5,\n",
        "        min_lr=1e-7,\n",
        "        verbose=1\n",
        "    )\n",
        "]\n",
        "\n",
        "print(f\"\\nüìã Training Configuration:\")\n",
        "print(f\"   - Epochs: 50 (with early stopping)\")\n",
        "print(f\"   - Batch size: 4\")\n",
        "print(f\"   - Learning rate: 0.0001\")\n",
        "print(f\"   - Steps per epoch: {len(train_generator)}\")\n",
        "print(f\"   - Total training samples per epoch: {len(train_generator) * 4}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## üöÄ 6. Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clear memory before training\n",
        "tf.keras.backend.clear_session()\n",
        "gc.collect()\n",
        "\n",
        "print(\"üöÄ Starting training...\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Start training\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    epochs=50,\n",
        "    callbacks=callbacks,\n",
        "    class_weight=class_weight_dict,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "print(\"\\nüéâ Training completed!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## üìä 7. Training Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot training history\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "# Plot loss\n",
        "ax1.plot(history.history['loss'], label='Training Loss', linewidth=2)\n",
        "ax1.set_title('Model Loss', fontsize=14, fontweight='bold')\n",
        "ax1.set_xlabel('Epoch')\n",
        "ax1.set_ylabel('Loss')\n",
        "ax1.legend()\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot accuracy\n",
        "ax2.plot(history.history['accuracy'], label='Training Accuracy', linewidth=2, color='orange')\n",
        "ax2.set_title('Model Accuracy', fontsize=14, fontweight='bold')\n",
        "ax2.set_xlabel('Epoch')\n",
        "ax2.set_ylabel('Accuracy')\n",
        "ax2.legend()\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print final metrics\n",
        "final_loss = history.history['loss'][-1]\n",
        "final_accuracy = history.history['accuracy'][-1]\n",
        "\n",
        "print(f\"\\nüìä Final Training Metrics:\")\n",
        "print(f\"   - Final Loss: {final_loss:.4f}\")\n",
        "print(f\"   - Final Accuracy: {final_accuracy:.4f} ({final_accuracy*100:.1f}%)\")\n",
        "print(f\"   - Total Epochs: {len(history.history['loss'])}\")\n",
        "\n",
        "# Random baseline comparison\n",
        "random_accuracy = 1.0 / 5  # 20% for 5 classes\n",
        "improvement = (final_accuracy - random_accuracy) / random_accuracy * 100\n",
        "print(f\"   - Random baseline: {random_accuracy:.1%}\")\n",
        "print(f\"   - Improvement over random: {improvement:.1f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## üß™ 8. Model Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test on a few random samples\n",
        "print(\"üß™ Testing model on random samples...\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "# Get a test batch\n",
        "test_batch_X, test_batch_y = train_generator[0]\n",
        "predictions = model.predict(test_batch_X, verbose=0)\n",
        "predicted_classes = np.argmax(predictions, axis=1)\n",
        "\n",
        "print(f\"Batch predictions:\")\n",
        "for i in range(len(test_batch_y)):\n",
        "    true_class = test_batch_y[i]\n",
        "    pred_class = predicted_classes[i]\n",
        "    confidence = predictions[i][pred_class]\n",
        "    \n",
        "    status = \"‚úÖ\" if true_class == pred_class else \"‚ùå\"\n",
        "    \n",
        "    print(f\"   {status} Sample {i+1}:\")\n",
        "    print(f\"      True: {action_classes[true_class]}\")\n",
        "    print(f\"      Pred: {action_classes[pred_class]} ({confidence:.3f})\")\n",
        "    print()\n",
        "\n",
        "# Calculate accuracy on this batch\n",
        "batch_accuracy = np.mean(test_batch_y == predicted_classes)\n",
        "print(f\"Batch accuracy: {batch_accuracy:.1%}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Comprehensive evaluation on more samples\n",
        "print(\"üîç Comprehensive evaluation...\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "all_predictions = []\n",
        "all_true_labels = []\n",
        "\n",
        "# Test on multiple batches (limit to avoid memory issues)\n",
        "num_test_batches = min(10, len(train_generator))\n",
        "print(f\"Testing on {num_test_batches} batches...\")\n",
        "\n",
        "for i in range(num_test_batches):\n",
        "    batch_X, batch_y = train_generator[i]\n",
        "    batch_pred = model.predict(batch_X, verbose=0)\n",
        "    batch_pred_classes = np.argmax(batch_pred, axis=1)\n",
        "    \n",
        "    all_predictions.extend(batch_pred_classes)\n",
        "    all_true_labels.extend(batch_y)\n",
        "\n",
        "all_predictions = np.array(all_predictions)\n",
        "all_true_labels = np.array(all_true_labels)\n",
        "\n",
        "# Calculate overall accuracy\n",
        "overall_accuracy = np.mean(all_true_labels == all_predictions)\n",
        "print(f\"\\nüìä Overall test accuracy: {overall_accuracy:.1%}\")\n",
        "\n",
        "# Per-class accuracy\n",
        "print(f\"\\nüìà Per-class accuracy:\")\n",
        "for class_idx in range(5):\n",
        "    class_mask = all_true_labels == class_idx\n",
        "    if np.sum(class_mask) > 0:\n",
        "        class_accuracy = np.mean(all_predictions[class_mask] == class_idx)\n",
        "        class_count = np.sum(class_mask)\n",
        "        print(f\"   {action_classes[class_idx]:<20}: {class_accuracy:.1%} ({class_count} samples)\")\n",
        "    else:\n",
        "        print(f\"   {action_classes[class_idx]:<20}: No samples\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## üíæ 9. Save Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save the trained model\n",
        "model_filename = \"merl_action_recognition_model.h5\"\n",
        "model.save(model_filename)\n",
        "print(f\"‚úÖ Model saved as: {model_filename}\")\n",
        "\n",
        "# Save training history\n",
        "import pickle\n",
        "history_filename = \"training_history.pkl\"\n",
        "with open(history_filename, 'wb') as f:\n",
        "    pickle.dump(history.history, f)\n",
        "print(f\"‚úÖ Training history saved as: {history_filename}\")\n",
        "\n",
        "# Save model info\n",
        "model_info = {\n",
        "    'architecture': 'TimeDistributed CNN + LSTM',\n",
        "    'sequence_length': 10,\n",
        "    'num_classes': 5,\n",
        "    'input_shape': (10, 224, 224, 3),\n",
        "    'final_accuracy': final_accuracy,\n",
        "    'final_loss': final_loss,\n",
        "    'total_parameters': trainable_params,\n",
        "    'training_samples': len(all_clip_data),\n",
        "    'action_classes': action_classes\n",
        "}\n",
        "\n",
        "info_filename = \"model_info.pkl\"\n",
        "with open(info_filename, 'wb') as f:\n",
        "    pickle.dump(model_info, f)\n",
        "print(f\"‚úÖ Model info saved as: {info_filename}\")\n",
        "\n",
        "print(f\"\\nüìÅ Generated files:\")\n",
        "print(f\"   - {model_filename} (model weights)\")\n",
        "print(f\"   - {history_filename} (training history)\")\n",
        "print(f\"   - {info_filename} (model metadata)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "nyoba"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üéØ TRAINING SUMMARY\n",
            "==================================================\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'all_clip_data' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[2], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124müéØ TRAINING SUMMARY\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m50\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset: 5 videos, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(\u001b[43mall_clip_data\u001b[49m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m clips\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel: TimeDistributed CNN + LSTM\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParameters: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainable_params\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[1;31mNameError\u001b[0m: name 'all_clip_data' is not defined"
          ]
        }
      ],
      "source": [
        "print(\"üéØ TRAINING SUMMARY\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"Dataset: 5 videos, {len(all_clip_data)} clips\")\n",
        "print(f\"Model: TimeDistributed CNN + LSTM\")\n",
        "print(f\"Parameters: {trainable_params:,}\")\n",
        "print(f\"Final Accuracy: {final_accuracy:.1%}\")\n",
        "print(f\"Training Time: {len(history.history['loss'])} epochs\")\n",
        "\n",
        "print(f\"\\nüöÄ Next Steps:\")\n",
        "print(f\"   1. ‚úÖ Model trained and saved\")\n",
        "print(f\"   2. üé¨ Test on new video inference\")\n",
        "print(f\"   3. üìä Add more videos if needed\")\n",
        "print(f\"   4. üîß Fine-tune hyperparameters\")\n",
        "print(f\"   5. üéØ Deploy for real-time inference\")\n",
        "\n",
        "if final_accuracy > 0.4:  # 40% for 5 classes is decent\n",
        "    print(f\"\\n‚úÖ Good performance! Model is ready for testing.\")\n",
        "elif final_accuracy > 0.25:  # 25% is better than random\n",
        "    print(f\"\\n‚ö†Ô∏è Moderate performance. Consider:\")\n",
        "    print(f\"   - Adding more training data\")\n",
        "    print(f\"   - Adjusting model architecture\")\n",
        "    print(f\"   - Tuning hyperparameters\")\n",
        "else:\n",
        "    print(f\"\\n‚ùå Low performance. Recommendations:\")\n",
        "    print(f\"   - Check data quality\")\n",
        "    print(f\"   - Increase training data\")\n",
        "    print(f\"   - Try different architecture\")\n",
        "    print(f\"   - Adjust learning rate\")\n",
        "\n",
        "print(f\"\\nüéâ Training complete! Model ready for deployment.\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
